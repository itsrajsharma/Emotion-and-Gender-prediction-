{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=13, sr=16000):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=sr)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        return mfccs_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_mfcc(file_path, n_mfcc=13, sr=16000):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=sr)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        return mfccs_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Directory containing the audio files\n",
    "audio_dir = 'D:\\\\ai ml prac\\\\actual\\\\data set\\\\processed audio'\n",
    "\n",
    "X = []\n",
    "y_gender = []\n",
    "y_emotion = []\n",
    "\n",
    "# Loop through each actor directory\n",
    "for actor in os.listdir(audio_dir):\n",
    "    actor_dir = os.path.join(audio_dir, actor)\n",
    "    if os.path.isdir(actor_dir):\n",
    "        # Extract the actor number from the directory name\n",
    "        try:\n",
    "            actor_number = int(actor.split('_')[1])\n",
    "            gender = 'male' if actor_number % 2 != 0 else 'female'  # Odd-numbered actors are male, even-numbered are female\n",
    "        except ValueError:\n",
    "            print(f\"Skipping directory with invalid format: {actor}\")\n",
    "            continue\n",
    "        \n",
    "        for file_name in os.listdir(actor_dir):\n",
    "            if file_name.endswith('.wav'):\n",
    "                file_path = os.path.join(actor_dir, file_name)\n",
    "                features = extract_mfcc(file_path)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y_gender.append(gender)\n",
    "                    # Assume emotion labels are embedded in file names, e.g., \"happy\" or \"sad\"\n",
    "                    emotion = file_name.split('_')[0]  # Adjust according to your naming convention\n",
    "                    y_emotion.append(emotion)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y_gender = np.array(y_gender)\n",
    "y_emotion = np.array(y_emotion)\n",
    "\n",
    "# Encode labels\n",
    "le_gender = LabelEncoder()\n",
    "le_emotion = LabelEncoder()\n",
    "y_gender_encoded = le_gender.fit_transform(y_gender)\n",
    "y_emotion_encoded = le_emotion.fit_transform(y_emotion)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_gender_train, y_gender_test, y_emotion_train, y_emotion_test = train_test_split(\n",
    "    X, y_gender_encoded, y_emotion_encoded, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - emotion_accuracy: 0.0000e+00 - gender_accuracy: 0.9358 - loss: 18.1175 \n",
      "Evaluation Results: [18.108379364013672, 0.0, 0.9375]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = model.evaluate(\n",
    "    X_test, {'gender': y_gender_test, 'emotion': y_emotion_test}\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\", evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - emotion_accuracy: 0.0000e+00 - gender_accuracy: 0.9358 - loss: 18.1175 \n",
      "Evaluation Results:\n",
      "Test Loss: 18.108379364013672\n",
      "Gender Loss: 0.0\n",
      "Emotion Loss: 0.9375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = model.evaluate(\n",
    "    X_test, {'gender': y_gender_test, 'emotion': y_emotion_test}\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Test Loss: {evaluation_results[0]}\")\n",
    "print(f\"Gender Loss: {evaluation_results[1]}\")\n",
    "print(f\"Emotion Loss: {evaluation_results[2]}\")\n",
    "\n",
    "# If accuracy metrics are not returned, you might need to add them to your model and re-evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'gender': 'sparse_categorical_crossentropy', 'emotion': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'gender': 'accuracy', 'emotion': 'accuracy'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - emotion_accuracy: 0.0000e+00 - gender_accuracy: 0.9358 - loss: 18.1175 \n",
      "Evaluation Results:\n",
      "Test Loss: 18.108379364013672\n",
      "Gender Loss: 0.0\n",
      "Emotion Loss: 0.9375\n",
      "Accuracy metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = model.evaluate(\n",
    "    X_test, {'gender': y_gender_test, 'emotion': y_emotion_test}\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Test Loss: {evaluation_results[0]}\")\n",
    "print(f\"Gender Loss: {evaluation_results[1]}\")\n",
    "print(f\"Emotion Loss: {evaluation_results[2]}\")\n",
    "\n",
    "# Optionally, check if the evaluation_results length matches the expected metrics\n",
    "if len(evaluation_results) >= 4:\n",
    "    print(f\"Gender Accuracy: {evaluation_results[3]}\")\n",
    "if len(evaluation_results) >= 5:\n",
    "    print(f\"Emotion Accuracy: {evaluation_results[4]}\")\n",
    "else:\n",
    "    print(\"Accuracy metrics are not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'gender': 'sparse_categorical_crossentropy', 'emotion': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'gender': 'accuracy', 'emotion': 'accuracy'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - emotion_accuracy: 0.0012 - gender_accuracy: 0.8988 - loss: 6.3168 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9134 - val_loss: 12.4517\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.0194 - gender_accuracy: 0.9295 - loss: 6.0683 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9351 - val_loss: 18.8242\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.0285 - gender_accuracy: 0.9421 - loss: 5.7716 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9048 - val_loss: 18.9595\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.0426 - gender_accuracy: 0.9434 - loss: 5.5317 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9481 - val_loss: 20.9841\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.0468 - gender_accuracy: 0.9419 - loss: 5.3117 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.8918 - val_loss: 24.0031\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - emotion_accuracy: 0.0895 - gender_accuracy: 0.9452 - loss: 5.0457 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.8745 - val_loss: 24.7151\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.0791 - gender_accuracy: 0.9314 - loss: 4.9469 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.8961 - val_loss: 27.0125\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.1115 - gender_accuracy: 0.9166 - loss: 4.8287 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9134 - val_loss: 27.0904\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.1194 - gender_accuracy: 0.9174 - loss: 4.6479 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9004 - val_loss: 28.7289\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - emotion_accuracy: 0.1536 - gender_accuracy: 0.9333 - loss: 4.4064 - val_emotion_accuracy: 0.0000e+00 - val_gender_accuracy: 0.9048 - val_loss: 30.2911\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, {'gender': y_gender_train, 'emotion': y_emotion_train},\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - emotion_accuracy: 0.0000e+00 - gender_accuracy: 0.9200 - loss: 30.9143 \n",
      "Evaluation Results:\n",
      "Test Loss: 30.90627098083496\n",
      "Gender Loss: 0.0\n",
      "Emotion Loss: 0.9305555820465088\n",
      "Accuracy metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = model.evaluate(\n",
    "    X_test, {'gender': y_gender_test, 'emotion': y_emotion_test}\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Test Loss: {evaluation_results[0]}\")\n",
    "print(f\"Gender Loss: {evaluation_results[1]}\")\n",
    "print(f\"Emotion Loss: {evaluation_results[2]}\")\n",
    "\n",
    "if len(evaluation_results) >= 4:\n",
    "    print(f\"Gender Accuracy: {evaluation_results[3]}\")\n",
    "if len(evaluation_results) >= 5:\n",
    "    print(f\"Emotion Accuracy: {evaluation_results[4]}\")\n",
    "else:\n",
    "    print(\"Accuracy metrics are not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gender (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emotion (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1440</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">47,520</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m896\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gender (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m66\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emotion (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1440\u001b[0m)      │     \u001b[38;5;34m47,520\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,688</span> (592.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m151,688\u001b[0m (592.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,562</span> (197.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,562\u001b[0m (197.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,126</span> (395.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m101,126\u001b[0m (395.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'gender': 'sparse_categorical_crossentropy', 'emotion': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'gender': metrics.SparseCategoricalAccuracy(), 'emotion': metrics.SparseCategoricalAccuracy()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('gender_emotion_classifier_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('gender_emotion_classifier_model.h5')\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=13, sr=16000):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        return mfccs_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_gender_emotion(audio_file):\n",
    "    mfccs = extract_mfcc(audio_file)\n",
    "    if mfccs is not None:\n",
    "        prediction = model.predict(np.expand_dims(mfccs, axis=0))\n",
    "        gender_pred = np.argmax(prediction[0])\n",
    "        emotion_pred = np.argmax(prediction[1])\n",
    "        return gender_pred, emotion_pred\n",
    "    return None, None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
